{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLADS 2019 - Deep Learning for NLP Applications: Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-D CNN's for Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Anatomy of a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a neural network revolves around the following objects:\n",
    "\n",
    "- Layer, which are combined into a network or model\n",
    "- Input data, and the corresponding targets\n",
    "- Loss function, which defines the feedback signal\n",
    "- Optimizer, which determines how the learning proceeds\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between network, layers, loss function and optimizer\n",
    "\n",
    "![title](figures/NN_Anatomy_color.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers: The building blocks of deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Layer is a data processing module that takes as input one or more tensors and outputs one or more tensors\n",
    "- More frequently, layers have a state: layer's weights (learned via the optimizer)\n",
    "- Types of Layers - Embedding, Densely connected, Dropout, convolutional, Pooling, Recurrent\n",
    "- Layers are almost like LEGO bricks of deep learning, that is made explicit by Keras\n",
    "- Layer compatibility refers to the fact that every layer will only accept input tensors of a certain shape and will return output tensors of a certain shape\n",
    "\n",
    "#### Building deep learing models in Keras is done by joining together compatible layers to form useful data-transformation pipelines\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential model is a linear stack of layers\n",
    "\n",
    "- You can create a Sequential model by passing a list of layer instances to the constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1118 16:09:17.434148 19176 deprecation_wrapper.py:119] From C:\\Users\\fatajadd\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1118 16:09:17.547518 19176 deprecation_wrapper.py:119] From C:\\Users\\fatajadd\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1118 16:09:17.570958 19176 deprecation_wrapper.py:119] From C:\\Users\\fatajadd\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 25,450\n",
      "Trainable params: 25,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "# Define the model architecture in terms of layers\n",
    "model = Sequential([\n",
    "    Dense(32, input_shape=(784,)),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "\n",
    "#print out the summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can also simply add layers via the .add() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 25,450\n",
      "Trainable params: 25,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture in terms of layers\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=784))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#Print out the summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ImDB Movie Reviews Classification with Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For preprocessing text\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# import sequential model\n",
    "from keras.models import Sequential\n",
    "\n",
    "#import needed layers\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "# import dataset\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up parameters for reading the textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "max_features: Number of words to consider as features (Vocabulary)\n",
    "\n",
    "maxlen: Cuts off text after this number of words \n",
    "\n",
    "'''\n",
    "\n",
    "# set parameters:\n",
    "max_features = 10000\n",
    "maxlen = 400\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Load the data as a list of integers\n",
    "'''\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "- The argument max_features = 5,000 means you will only keep the top 5,000 most frequently used words in the training data, and rare words will be discarded. \n",
    "\n",
    "- The argument maxlen = 400 means we will only keep the first 400 words in the review and postpad (or prepad) if the review is shorter than 400 words\n",
    "\n",
    "- x_train and x_test are lists of reviews; each review is a list of word indices (encoding a sequence of words)\n",
    "\n",
    "- y_train and y_test are lists of 0's and 1's (0: negative and 1: positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example to illustrate the preprocessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](figures/preprocess_text.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   7,    4,  118,  785,  189,  108,  126,   93,    2,   16,  540,\n",
       "        324,   23,    6,  364,  352,   21,   14,    9,   93,   56,   18,\n",
       "         11,  230,   53,  771,   74,   31,   34,    4, 2834,    7,    4,\n",
       "         22,    5,   14,   11,  471,    9,    2,   34,    4,  321,  487,\n",
       "          5,  116,   15, 6584,    4,   22,    9,    6, 2286,    4,  114,\n",
       "       2679,   23,  107,  293, 1008, 1172,    5,  328, 1236,    4, 1375,\n",
       "        109,    9,    6,  132,  773,    2, 1412,    8, 1172,   18, 7865,\n",
       "         29,    9,  276,   11,    6, 2768,   19,  289,  409,    4, 5341,\n",
       "       2140,    2,  648, 1430,    2, 8914,    5,   27, 3000, 1432, 7130,\n",
       "        103,    6,  346,  137,   11,    4, 2768,  295,   36, 7740,  725,\n",
       "          6, 3208,  273,   11,    4, 1513,   15, 1367,   35,  154,    2,\n",
       "        103,    2,  173,    7,   12,   36,  515, 3547,   94, 2547, 1722,\n",
       "          5, 3547,   36,  203,   30,  502,    8,  361,   12,    8,  989,\n",
       "        143,    4, 1172, 3404,   10,   10,  328, 1236,    9,    6,   55,\n",
       "        221, 2989,    5,  146,  165,  179,  770,   15,   50,  713,   53,\n",
       "        108,  448,   23,   12,   17,  225,   38,   76, 4397,   18,  183,\n",
       "          8,   81,   19,   12,   45, 1257,    8,  135,   15,    2,  166,\n",
       "          4,  118,    7,   45,    2,   17,  466,   45,    2,    4,   22,\n",
       "        115,  165,  764, 6075,    5, 1030,    8, 2973,   73,  469,  167,\n",
       "       2127,    2, 1568,    6,   87,  841,   18,    4,   22,    4,  192,\n",
       "         15,   91,    7,   12,  304,  273, 1004,    4, 1375, 1172, 2768,\n",
       "          2,   15,    4,   22,  764,   55, 5773,    5,   14, 4233, 7444,\n",
       "          4, 1375,  326,    7,    4, 4760, 1786,    8,  361, 1236,    8,\n",
       "        989,   46,    7,    4, 2768,   45,   55,  776,    8,   79,  496,\n",
       "         98,   45,  400,  301,   15,    4, 1859,    9,    4,  155,   15,\n",
       "         66,    2,   84,    5,   14,   22, 1534,   15,   17,    4,  167,\n",
       "          2,   15,   75,   70,  115,   66,   30,  252,    7,  618,   51,\n",
       "          9, 2161,    4, 3130,    5,   14, 1525,    8, 6584,   15,    2,\n",
       "        165,  127, 1921,    8,   30,  179, 2532,    4,   22,    9,  906,\n",
       "         18,    6,  176,    7, 1007, 1005,    4, 1375,  114,    4,  105,\n",
       "         26,   32,   55,  221,   11,   68,  205,   96,    5,    4,  192,\n",
       "         15,    4,  274,  410,  220,  304,   23,   94,  205,  109,    9,\n",
       "         55,   73,  224,  259, 3786,   15,    4,   22,  528, 1645,   34,\n",
       "          4,  130,  528,   30,  685,  345,   17,    4,  277,  199,  166,\n",
       "        281,    5, 1030,    8,   30,  179, 4442,  444,    2,    9,    6,\n",
       "        371,   87,  189,   22,    5,   31,    7,    4,  118,    7,    4,\n",
       "       2068,  545, 1178,  829])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing out one particular training example\n",
    "\n",
    "x_train[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how you can decode it back to get the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "onset ==> 15628\n",
      "cassie ==> 8872\n",
      "tkotsw ==> 63333\n",
      "oakhurst ==> 42115\n",
      "dailey ==> 34722\n",
      "collusion ==> 40661\n"
     ]
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "print(type(word_index))\n",
    "\n",
    "count = 0\n",
    "for key, val in word_index.items():\n",
    "    print(key, \"==>\", val)\n",
    "    count = count + 1\n",
    "    if (count > 5):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = dict([(value,key) for (key, value) in word_index.items()])\n",
    "decoded_review = ' '.join([reverse_word_index.get(i-3,'?') for i in x_train[0]])\n",
    "print(decoded_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training and evaluating a simple 1D convnet on the IMDB data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras offers an __Embedding layer__ that can be used for neural networks on text data.\n",
    "\n",
    "It requires that the input data be integer encoded, so that each word is represented by a unique integer. This data preparation step can be performed using the Tokenizer API also provided with Keras.\n",
    "\n",
    "The Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset.\n",
    "\n",
    "It is a flexible layer that can be used in a variety of ways, such as:\n",
    "\n",
    "- It can be used alone to learn a word embedding that can be saved and used in another model later.\n",
    "- It can be used as part of a deep learning model where the embedding is learned along with the model itself.\n",
    "- It can be used to load a pre-trained word embedding model, a type of transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Defining the params of the CNN\n",
    "\n",
    "embedding dims: Dimensionality of the vector representing each word\n",
    "\n",
    "filters: Number of convolution filters used\n",
    "\n",
    "kernel_size: length of the 1-D conv filter\n",
    "\n",
    "epochs: Number of cyclings through the train set\n",
    "\n",
    "batch_size: Weight vector Updates are made after cycling through these many examples\n",
    "'''\n",
    "\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 2\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustration of the network architecture we will be using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](figures/network_architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specify model...\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 400, 50)           500000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 398, 250)          37750     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 600,751\n",
      "Trainable params: 600,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Specify model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "The Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments:\n",
    "\n",
    "It must specify 3 arguments:\n",
    "\n",
    "- __input_dim__: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n",
    "- __output_dim__: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.\n",
    "- __input_length__: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustration of the embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](figures/embedding_layer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1118 16:32:36.370974 19176 deprecation_wrapper.py:119] From C:\\Users\\fatajadd\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/6\n",
      "25000/25000 [==============================] - 111s 4ms/step - loss: 0.3762 - acc: 0.8196 - val_loss: 0.3240 - val_acc: 0.8620\n",
      "Epoch 2/6\n",
      "25000/25000 [==============================] - 114s 5ms/step - loss: 0.1775 - acc: 0.9326 - val_loss: 0.2661 - val_acc: 0.8916\n",
      "Epoch 3/6\n",
      "25000/25000 [==============================] - 120s 5ms/step - loss: 0.0694 - acc: 0.9776 - val_loss: 0.3472 - val_acc: 0.8826\n",
      "Epoch 4/6\n",
      "25000/25000 [==============================] - 111s 4ms/step - loss: 0.0269 - acc: 0.9915 - val_loss: 0.4714 - val_acc: 0.8756\n",
      "Epoch 5/6\n",
      "25000/25000 [==============================] - 106s 4ms/step - loss: 0.0135 - acc: 0.9954 - val_loss: 0.5650 - val_acc: 0.8759\n",
      "Epoch 6/6\n",
      "25000/25000 [==============================] - 92s 4ms/step - loss: 0.0097 - acc: 0.9967 - val_loss: 0.6390 - val_acc: 0.8801\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Plotting training and validation accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The call to model.fit() returns a history object\n",
    "- This has a member \"history\", which is a dictionary containing everything that happened during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.81728000000000001, 0.93640000000000001, 0.97819999999999996, 0.99424000000000001, 0.99872000000000005, 0.99531999999999998]\n"
     ]
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "print(history_dict['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "\n",
    "plt.plot(epochs, acc_values, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating results on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 2s 79us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.63385873230099676, 0.87512000000000001]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
